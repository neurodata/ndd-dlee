{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kernels\n",
    "\n",
    "We build our simple kernels using global, node-wise, and edge-wise properties of the network. Both datasets have the shape $(N, n, n)$ where $N$ is the number of networks in the dataset and $n$ is the number of nodes in each network. Each graph is represented by an $(n$ x $n)$ adjacency matrix, where the existence of an edge between node $i$ and node $j$ is denoted by a nonzero value at the $i,j$ th element. \n",
    "\n",
    "# For Matched Networks\n",
    "\n",
    "Attached is the code for calculating the dissimilarity matrix using different kernels for matched networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# HIDE CODE\n",
    "import numpy as np\n",
    "\n",
    "def calculate_dissim(graphs, method=\"density\", norm=None, normalize=True):\n",
    "    \"\"\" Calculate the dissimilarity matrix using the input kernel. \"\"\"\n",
    "    glob = False\n",
    "    node = False\n",
    "    edge = False\n",
    "\n",
    "    if method == \"density\":\n",
    "        glob = True\n",
    "        num_nodes = graphs.shape[1]\n",
    "        num_edges_possible = num_nodes ** 2 - num_nodes\n",
    "\n",
    "        metric = np.zeros(len(graphs))\n",
    "        for i, graph in enumerate(graphs):\n",
    "            num_edges = np.count_nonzero(graph)\n",
    "            metric[i] = num_edges / num_edges_possible\n",
    "    \n",
    "    elif method == \"avgedgeweight\":\n",
    "        glob = True\n",
    "        metric = np.zeros(len(graphs))\n",
    "        for i, graph in enumerate(graphs):\n",
    "            num_edges = np.count_nonzero(graph)\n",
    "            metric[i] = np.sum(graph) / num_edges\n",
    "    \n",
    "    elif method == \"avgadjmatrix\":\n",
    "        glob = True\n",
    "        metric = np.zeros(len(graphs))\n",
    "        for i, graph in enumerate(graphs):\n",
    "            metric[i] = np.average(graph)   \n",
    "\n",
    "    elif method == \"degree\":\n",
    "        node = True\n",
    "        metric = np.zeros((graphs.shape[0], graphs.shape[1]))\n",
    "        for i, graph in enumerate(graphs):\n",
    "            for j, row in enumerate(graph):\n",
    "                metric[i, j] = np.count_nonzero(row)\n",
    "    \n",
    "    elif method == \"strength\":\n",
    "        node = True\n",
    "        metric = np.zeros((graphs.shape[0], graphs.shape[1]))\n",
    "        for i, graph in enumerate(graphs):\n",
    "            for j, row in enumerate(graph):\n",
    "                metric[i, j] = np.sum(row)\n",
    "\n",
    "    elif method == \"edgeweight\":\n",
    "        edge = True\n",
    "        metric = graphs\n",
    "    \n",
    "    else:\n",
    "        print(\"Not a valid kernel name.\")\n",
    "    \n",
    "    dissim_matrix = np.zeros((len(graphs), len(graphs)))\n",
    "\n",
    "    for i, metric1 in enumerate(metric):\n",
    "        for j, metric2 in enumerate(metric):\n",
    "            if glob and norm == None:\n",
    "                diff = np.abs(metric1- metric2)\n",
    "            elif (node or edge) and norm == \"l1\":\n",
    "                diff = np.linalg.norm(metric1 - metric2, ord=1)\n",
    "            elif (node or edge) and norm == \"l2\":\n",
    "                diff = np.linalg.norm(metric1 - metric2, ord=2)\n",
    "            else:\n",
    "                print(\"L1, L2 norms only apply to node or edge-wise kernels.\")\n",
    "            \n",
    "            dissim_matrix[j, i] = diff\n",
    "\n",
    "    if normalize:\n",
    "        dissim_matrix = dissim_matrix / np.max(dissim_matrix)\n",
    "    \n",
    "    return dissim_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Properties \n",
    "\n",
    "For each of these kernels, each element of the $(N$ x $N)$ dissimilarity matrix is the absolute difference between the metrics, where $N$ is the number of networks in the dataset.\n",
    "\n",
    "### Density\n",
    "Graph density refers to the ratio between the number of edges present in the graph and the number of possible edges and is calculated by \n",
    "```{math}\n",
    "    \\text{Density} = \\frac{\\text{Number of Edges Present}}{n(n-1)}\n",
    "```\n",
    "where $n$ is the number of nodes in the graph. \n",
    "\n",
    "### Average Edge Weight\n",
    "To calculate average edge weight, we sum all the nonzero values in the adjacency matrix and divide by the number of edges present.\n",
    "```{math}\n",
    "    \\text{Average Edge Weight} = \\frac{\\text{Sum of Edge Weights}}{\\text{Number of Edges Present}}\n",
    "```\n",
    "\n",
    "### Average of the Adjacency Matrix\n",
    "To calculate the average of the adjacency matrix, we sum all the nonzero values in the adjacency matrix and divide by the total number of possible edges. Note that if the graph is unweighted, the average of the adjacency matrix would be the same as graph density.\n",
    "```{math}\n",
    "    \\text{Average of Adj. Matrix} = \\frac{\\text{Sum of Edge Weights}}{n(n-1)}\n",
    "```\n",
    "where $n$ is the number of nodes in the graph.\n",
    "\n",
    "## Node-wise Properties\n",
    "\n",
    "For each of these kernels, two $(N$ x $N)$ dissimilarity matrices are constructed. Each element of the dissimilarity matrix is the $L^1$-norm and $L^2$-norm of the difference vector between the metrics. \n",
    "\n",
    ":::{note}\n",
    "The $L^1$-norm is the sum of the magnitude of the vector. Formally, \n",
    "```{math}\n",
    "    |\\boldsymbol{x}|_1 = \\sum_{i=1}^n |x_i|\n",
    "```\n",
    "where $x_i$ for $i = 1, 2, ..., n$ are the elements of the vector $\\boldsymbol{x}$ {cite}`weisstein2022l1norm`.\n",
    ":::\n",
    "\n",
    ":::{note}\n",
    "The $L^2$-norm is the Euclidean distance from the origin to the vector. Formally, \n",
    "```{math}\n",
    "    |\\boldsymbol{x}|_2 = \\sqrt{\\sum_{i=1}^n |x_i|^2}\n",
    "```\n",
    "where $x_i$ for $i = 1, 2, ..., n$ are the elements of the vector $\\boldsymbol{x}$ {cite}`weisstein2022l2norm`.\n",
    ":::\n",
    "\n",
    "### Node Degree\n",
    "The degree of a node is the number of edges a node has. In an $(n$ x $n)$ adjacency matrix $A$, the degree of node $i$ is calculated by $\\sum_{j=1}^n a_{j}$ where $a_{j}=1$ if $A_{ij} \\neq 0$ and $a_{j}=0$ if $A_{ij} = 0$. Thus for each graph, we have an $(i$ x $1)$ vector where each element is the degree of node $i$, and the dissimilarity matrix is constructed based on these vectors. \n",
    "\n",
    "### Node Strength\n",
    "The strength of a node is the sum of the weights of the edges a node has. In an $(n$ x $n)$ adjacency matrix $A$, the degree of node $i$ is calculated by $\\sum_{j=1}^n A_{ij}$. Thus for each graph, we have an $(i$ x $1)$ vector where each element is the strength of node $i$, and the dissimilarity matrix is constructed based on these vectors.\n",
    "\n",
    "## Edge-wise Properties \n",
    "Similar to the kernels above, two $(N$ x $N)$ dissimilarity matrices are constructed based on the $L^1$-matrix norm and $L^2$-matrix norm of the difference matrix between the metrics. \n",
    "\n",
    ":::{note}\n",
    "The $L^1$-matrix norm is also called the maximum absolute column sum norm and is calculated by\n",
    "```{math}\n",
    "    ||\\boldsymbol{A}_1|| = \\text{max}_j \\sum_{i=1}^n |A_ij|\n",
    "``` \n",
    "{cite}`weisstein2022matrixnorm`.\n",
    ":::\n",
    "\n",
    ":::{note}\n",
    "The $L^2$-matrix norm is also called the spectral norm and is calculated by\n",
    "```{math}\n",
    "    ||\\boldsymbol{A}_2|| = (\\text{max eigenvalue of} A^H A)^{1/2}\n",
    "```\n",
    "where $A^H$ is the conjugate transpose matrix. In our case, matrix $A$ is an adjacency matrix and thus always real, so $A^H = A^T$ {cite}`weisstein2022matrixnorm`.\n",
    ":::\n",
    "\n",
    "### Edge Weight\n",
    "For each pair of $(n$ x $n)$ graphs, the difference in edge weights is calculated by the difference in their adjacency matrices, yielding an $(n$ x $n$) difference matrix. Then the dissimilarity matrix is constructed based on the $L^1$ and $L^2$-matrix norm of these matrices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For Unmatched Networks\n",
    "\n",
    "The kernels we use above assume the nodes are matched. However, when we have no known matching of the nodes, we cannot use kernels based on the node-wise or edge-wise properties of the network because we cannot calculate a difference vector or matrix directly and calculate its $L^1$ and $L^2$ norm. Thus, we flatten the difference vector or matrix for those kernels and find the [two-sample Kolmogorov-Smirnov test statistic](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.ks_2samp.html) for the two \"distributions\". The metrics remain the same. The modified code is attached below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# HIDE CODE\n",
    "from scipy.stats import ks_2samp\n",
    "\n",
    "def calculate_dissim_unmatched(graphs, method=\"degree\", normalize=True):\n",
    "    \"\"\" Calculate the dissimilarity matrix using the input kernel. \"\"\"\n",
    "    if method == \"degree\":\n",
    "        metric = np.zeros((graphs.shape[0], graphs.shape[1]))\n",
    "        for i, graph in enumerate(graphs):\n",
    "            for j, node in enumerate(graph):\n",
    "                metric[i, j] = np.count_nonzero(node)\n",
    "    \n",
    "    elif method == \"strength\":\n",
    "        metric = np.zeros((graphs.shape[0], graphs.shape[1]))\n",
    "        for i, graph in enumerate(graphs):\n",
    "            for j, node in enumerate(graph):\n",
    "                metric[i, j] = np.sum(node)\n",
    "\n",
    "    elif method == \"edgeweight\":\n",
    "        metric = []\n",
    "        for graph in graphs:\n",
    "            metric.append(np.ravel(np.nonzero(graph)))\n",
    "            \n",
    "    else:\n",
    "        print(\"Not a valid kernel name.\")\n",
    "    \n",
    "    dissim_matrix = np.zeros((len(graphs), len(graphs)))\n",
    "    for i, metric1 in enumerate(metric):\n",
    "        for j, metric2 in enumerate(metric):\n",
    "            diff, _ = ks_2samp(np.array(metric1), np.array(metric2), mode='asymp')\n",
    "            dissim_matrix[i, j] = diff\n",
    "            \n",
    "    if normalize:\n",
    "        dissim_matrix = dissim_matrix / np.max(dissim_matrix)\n",
    "    \n",
    "    return dissim_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Established Kernels\n",
    "The following kernels use existing algorithms and/or equations that are much more complex and computationally expensive than those described above. We use them as a point of comparison for the performance for our \"simple\" kernels. \n",
    "\n",
    "## Laplacian Spectral Distance\n",
    "We compute the [I-DAD Laplacian](https://microsoft.github.io/graspologic/latest/reference/reference/utils.html#graspologic.utils.to_laplacian) of each graph, which is calculated by\n",
    "```{math}\n",
    "L = I - D_i A D_i\n",
    "```\n",
    "where $D$ is the diagonal matrix of degrees of node $i$ raised to the -1/2 power, $I$ is the identity matrix, and $A$ is the adjacency matrix.\n",
    "\n",
    "Graphs can often be characterized by its Laplacian eigenspectrum, which makes this a useful metric to consider when assessing similarity between graphs {cite}`das2004lap`. The I-DAD Laplacian is particularly useful because all its eigenvalues are real, and for undirected graphs, all eigenvalues are guaranteed to be in the range [0, 2] {cite}`suarez2022taxonomy`. \n",
    "\n",
    "We implement the Laplacian spectral kernel by applying pass-to-ranks transformations on the networks, sorting the eigenvalues of the graph Laplacian without any smoothing, then calculating the $L^2$-norm of the difference vector to construct the dissimilarity matrix. \n",
    "\n",
    ":::{note}\n",
    "Using the code attached below, there are several ways we can derive and use the Laplacian eigenspectrum. We can transform the graphs through [pass-to-ranks](https://microsoft.github.io/graspologic/latest/reference/reference/utils.html#graspologic.utils.pass_to_ranks) or binarization before finding its graph Laplacian, and we can simply sort its eigenvalues or smooth them using a Gaussian kernel to construct the eigenspectrum. We also have multiple ways to quantify dissimilarity, such as calculating the [cosine difference](https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.distance.cosine.html) between two eigenspectra, or by finding the $L^1$-norm, or $L^2$-norm of the difference vector between two eigenspectra. The implementation we state above is based on which yields the highest discriminiability value on the matched networks. More details can be found in the {doc}`Laplacian <laplacian.ipynb>` section in the Appendix.\n",
    ":::\n",
    "\n",
    "We use the Laplacian spectral kernel for both matched and unmatched networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# HIDE CODE\n",
    "import numpy as np\n",
    "from graspologic.utils import pass_to_ranks, to_laplacian\n",
    "from sklearn.neighbors import KernelDensity\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "def laplacian_dissim(graphs, transform: str=None, metric: str='l2', smooth_eigvals: bool=False, \\\n",
    "    normalize=True):\n",
    "    if transform == 'pass-to-ranks':\n",
    "        for i, graph in enumerate(graphs):\n",
    "            graph = pass_to_ranks(graph)\n",
    "            graphs[i] = graph\n",
    "    elif transform == 'binarize':\n",
    "        graphs[graphs != 0] = 1\n",
    "    elif transform == None:\n",
    "        graphs = graphs\n",
    "    else:\n",
    "        print('Supported transformations are \"pass-to-ranks\" (simple-nonzero), \"binarize\", or None.')\n",
    "    \n",
    "    eigs = []\n",
    "    for i, graph in enumerate(graphs):\n",
    "        # calculate laplacian\n",
    "        lap = to_laplacian(graph, 'I-DAD')\n",
    "\n",
    "        # find and sort eigenvalues\n",
    "        w = np.linalg.eigvals(lap)\n",
    "        w = np.sort(w)\n",
    "\n",
    "        if smooth_eigvals:\n",
    "            kde = KernelDensity(kernel='gaussian', bandwidth=0.015).fit(w.reshape(-1, 1))\n",
    "            xs = np.linspace(0, 2, 2000)\n",
    "            xs = xs[:, np.newaxis]\n",
    "            log_dens = kde.score_samples(xs)\n",
    "            eigs.append(np.exp(log_dens))\n",
    "        else:\n",
    "            eigs.append(w)\n",
    "\n",
    "    dissim_matrix = np.zeros((len(graphs), len(graphs)))\n",
    "    for i, eig1 in enumerate(eigs):\n",
    "        for j, eig2 in enumerate(eigs):\n",
    "            if metric == 'cosine':\n",
    "                diff = cosine(eig1, eig2)\n",
    "            elif metric == 'l1':\n",
    "                diff = np.linalg.norm(eig1 - eig2, ord=1)\n",
    "            elif metric == 'l2':\n",
    "                diff = np.linalg.norm(eig1 - eig2, ord=2)\n",
    "            dissim_matrix[i, j] = diff\n",
    "\n",
    "    if normalize:\n",
    "        dissim_matrix = dissim_matrix / np.max(dissim_matrix)\n",
    "    \n",
    "    return dissim_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For Matched Networks - Omnibus Embedding\n",
    "\n",
    "We generate [omnibus embeddings](https://microsoft.github.io/graspologic/latest/reference/reference/embed.html#graspologic.embed.OmnibusEmbed) of the matched networks, then calculate the Frobenius norm of the difference between two embeddings to construct the dissimilarity matrix.\n",
    "\n",
    ":::{note}\n",
    "The Frobenius norm of a matrix $A$ is given by \n",
    "```{math}\n",
    "||A||_F = (\\sum_{i,j} |A_{ij}|^2)^{1/2}\n",
    "```\n",
    "{cite}`golub1985matrixcomp`.\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# HIDE CODE\n",
    "from graspologic.embed import OmnibusEmbed\n",
    "\n",
    "# embed using Omni\n",
    "embedder = OmnibusEmbed(n_elbows=4)\n",
    "omni_embedding = embedder.fit_transform(graphs)\n",
    "\n",
    "omni_matrix = np.zeros((len(graphs), len(graphs)))\n",
    "for i, embedding1 in enumerate(omni_embedding):\n",
    "    for j, embedding2 in enumerate(omni_embedding):\n",
    "        dist = np.linalg.norm(embedding1 - embedding2, ord=\"fro\")\n",
    "        omni_matrix[i, j] = dist\n",
    "\n",
    "# scale values from 0 to 1\n",
    "scaled_omni_dissim = omni_matrix / np.max(omni_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For Unmatched Networks - Latent Distribution Test Statistic\n",
    "\n",
    "The [latent distribution test](https://microsoft.github.io/graspologic/latest/reference/reference/inference.html#graspologic.inference.latent_distribution_test) is a two-sample hypothesis test for whether or not two graphs have the same distributions of latent positions. This test does not assume any known matching, as it can use alignment techniques such as [seedless-procrustes](https://microsoft.github.io/graspologic/latest/reference/reference/align.html#graspologic.align.SeedlessProcrustes) to find the best orthogonal alignment between two embeddings.\n",
    "\n",
    "First we generate [adjacency spectral embeddings](https://microsoft.github.io/graspologic/latest/reference/reference/embed.html#graspologic.embed.AdjacencySpectralEmbed) of the largest connected component of the unmatched graphs, then calculate the latent distribution test statistic for every pair to construct the dissimilarity matrix using parameters shown in the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# HIDE CODE\n",
    "from graspologic.inference import latent_distribution_test\n",
    "from graspologic.embed import AdjacencySpectralEmbed\n",
    "from graspologic.utils import largest_connected_component\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "ase_graphs = []\n",
    "for i, graph in enumerate(graphs):\n",
    "    lcc_graph = largest_connected_component(graph)\n",
    "    ase_graph = AdjacencySpectralEmbed(n_components=4).fit_transform(lcc_graph)\n",
    "    ase_graphs.append(ase_graph)\n",
    "\n",
    "# calculate alignments\n",
    "latent_dissim = np.zeros((len(ase_graphs), len(ase_graphs)))\n",
    "Qs = []\n",
    "for j in tqdm(range(0, len(ase_graphs) - 1)):\n",
    "    for i in range(j+1, len(ase_graphs)):\n",
    "        statistic, _, misc_dict = latent_distribution_test(ase_graphs[i], ase_graphs[j], test='mgc', metric='euclidean', \\\n",
    "            n_bootstraps=0, align_type='seedless_procrustes', input_graph=False)\n",
    "        latent_dissim[i, j] = statistic\n",
    "        Qs.append(misc_dict['Q'])\n",
    "\n",
    "# scale values from 0 to 1 and make matrix symmetric\n",
    "scaled_latent_dissim = latent_dissim / np.max(latent_dissim)\n",
    "scaled_latent_dissim = scaled_latent_dissim + scaled_latent_dissim.T\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "```{bibliography}\n",
    ":filter: docname in docnames\n",
    ":style: unsrt\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7e8666d8190e1c4ba3ba575221b29e27608238cc9068675599c456fa53e3686b"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('bilateral')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
